RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=97, bias=True)
    (1): CELU(alpha=1.0)
    (2): Linear(in_features=97, out_features=42, bias=True)
    (3): Hardsigmoid()
    (4): Linear(in_features=42, out_features=131, bias=True)
    (5): Sigmoid()
    (6): Linear(in_features=131, out_features=29, bias=True)
    (7): PReLU(num_parameters=1)
    (8): Linear(in_features=29, out_features=31, bias=True)
    (9): SiLU()
    (10): Linear(in_features=31, out_features=129, bias=True)
    (11): ELU(alpha=1.0)
    (12): Linear(in_features=129, out_features=64, bias=True)
    (13): SELU()
    (14): Linear(in_features=64, out_features=24, bias=True)
    (15): Softshrink(0.5)
    (16): Linear(in_features=24, out_features=143, bias=True)
    (17): ReLU6()
    (18): Linear(in_features=143, out_features=9, bias=True)
    (19): RReLU(lower=0.125, upper=0.3333333333333333)
    (20): Linear(in_features=9, out_features=36, bias=True)
    (21): Softshrink(0.5)
    (22): Linear(in_features=36, out_features=39, bias=True)
    (23): LeakyReLU(negative_slope=0.01)
    (24): Linear(in_features=39, out_features=47, bias=True)
    (25): RReLU(lower=0.125, upper=0.3333333333333333)
    (26): Linear(in_features=47, out_features=1, bias=True)
    (27): Hardtanh(min_val=-1.0, max_val=1.0)
  )
)
RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=65, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=65, out_features=86, bias=True)
    (3): SELU()
    (4): Linear(in_features=86, out_features=60, bias=True)
    (5): Tanh()
    (6): Linear(in_features=60, out_features=13, bias=True)
    (7): ELU(alpha=1.0)
    (8): Linear(in_features=13, out_features=20, bias=True)
    (9): Hardtanh(min_val=-1.0, max_val=1.0)
    (10): Linear(in_features=20, out_features=100, bias=True)
    (11): ELU(alpha=1.0)
    (12): Linear(in_features=100, out_features=43, bias=True)
    (13): Sigmoid()
    (14): Linear(in_features=43, out_features=50, bias=True)
    (15): Tanhshrink()
    (16): Linear(in_features=50, out_features=65, bias=True)
    (17): ELU(alpha=1.0)
    (18): Linear(in_features=65, out_features=15, bias=True)
    (19): LeakyReLU(negative_slope=0.01)
    (20): Linear(in_features=15, out_features=4, bias=True)
    (21): Hardswish()
    (22): Linear(in_features=4, out_features=51, bias=True)
    (23): ReLU6()
    (24): Linear(in_features=51, out_features=35, bias=True)
    (25): GELU(approximate='none')
    (26): Linear(in_features=35, out_features=85, bias=True)
    (27): ReLU6()
    (28): Linear(in_features=85, out_features=84, bias=True)
    (29): Hardtanh(min_val=-1.0, max_val=1.0)
    (30): Linear(in_features=84, out_features=41, bias=True)
    (31): Softplus(beta=1, threshold=20)
    (32): Linear(in_features=41, out_features=50, bias=True)
    (33): Hardshrink(0.5)
    (34): Linear(in_features=50, out_features=1, bias=True)
    (35): RReLU(lower=0.125, upper=0.3333333333333333)
  )
)
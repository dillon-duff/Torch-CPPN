RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=77, bias=True)
    (1): Sigmoid()
    (2): Linear(in_features=77, out_features=68, bias=True)
    (3): Mish()
    (4): Linear(in_features=68, out_features=52, bias=True)
    (5): RReLU(lower=0.125, upper=0.3333333333333333)
    (6): Linear(in_features=52, out_features=19, bias=True)
    (7): GELU(approximate='none')
    (8): Linear(in_features=19, out_features=70, bias=True)
    (9): RReLU(lower=0.125, upper=0.3333333333333333)
    (10): Linear(in_features=70, out_features=79, bias=True)
    (11): Hardtanh(min_val=-1.0, max_val=1.0)
    (12): Linear(in_features=79, out_features=37, bias=True)
    (13): RReLU(lower=0.125, upper=0.3333333333333333)
    (14): Linear(in_features=37, out_features=23, bias=True)
    (15): Hardsigmoid()
    (16): Linear(in_features=23, out_features=88, bias=True)
    (17): LogSigmoid()
    (18): Linear(in_features=88, out_features=27, bias=True)
    (19): Hardswish()
    (20): Linear(in_features=27, out_features=98, bias=True)
    (21): SELU()
    (22): Linear(in_features=98, out_features=87, bias=True)
    (23): Hardswish()
    (24): Linear(in_features=87, out_features=1, bias=True)
    (25): LeakyReLU(negative_slope=0.01)
  )
)
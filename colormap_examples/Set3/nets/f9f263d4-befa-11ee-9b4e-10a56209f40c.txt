RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=59, bias=True)
    (1): Softplus(beta=1, threshold=20)
    (2): Linear(in_features=59, out_features=59, bias=True)
    (3): SELU()
    (4): Linear(in_features=59, out_features=29, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
    (6): Linear(in_features=29, out_features=87, bias=True)
    (7): Mish()
    (8): Linear(in_features=87, out_features=141, bias=True)
    (9): LeakyReLU(negative_slope=0.01)
    (10): Linear(in_features=141, out_features=39, bias=True)
    (11): RReLU(lower=0.125, upper=0.3333333333333333)
    (12): Linear(in_features=39, out_features=9, bias=True)
    (13): Sigmoid()
    (14): Linear(in_features=9, out_features=56, bias=True)
    (15): ReLU()
    (16): Linear(in_features=56, out_features=72, bias=True)
    (17): RReLU(lower=0.125, upper=0.3333333333333333)
    (18): Linear(in_features=72, out_features=27, bias=True)
    (19): ReLU()
    (20): Linear(in_features=27, out_features=108, bias=True)
    (21): SiLU()
    (22): Linear(in_features=108, out_features=35, bias=True)
    (23): ReLU6()
    (24): Linear(in_features=35, out_features=1, bias=True)
    (25): Hardtanh(min_val=-1.0, max_val=1.0)
  )
)
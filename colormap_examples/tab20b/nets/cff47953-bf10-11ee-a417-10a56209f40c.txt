RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=2, bias=True)
    (1): Tanhshrink()
    (2): Linear(in_features=2, out_features=41, bias=True)
    (3): Hardsigmoid()
    (4): Linear(in_features=41, out_features=47, bias=True)
    (5): ELU(alpha=1.0)
    (6): Linear(in_features=47, out_features=31, bias=True)
    (7): ReLU()
    (8): Linear(in_features=31, out_features=38, bias=True)
    (9): CELU(alpha=1.0)
    (10): Linear(in_features=38, out_features=29, bias=True)
    (11): Mish()
    (12): Linear(in_features=29, out_features=70, bias=True)
    (13): SELU()
    (14): Linear(in_features=70, out_features=44, bias=True)
    (15): LeakyReLU(negative_slope=0.01)
    (16): Linear(in_features=44, out_features=65, bias=True)
    (17): Softshrink(0.5)
    (18): Linear(in_features=65, out_features=5, bias=True)
    (19): Hardtanh(min_val=-1.0, max_val=1.0)
    (20): Linear(in_features=5, out_features=18, bias=True)
    (21): SELU()
    (22): Linear(in_features=18, out_features=50, bias=True)
    (23): Sigmoid()
    (24): Linear(in_features=50, out_features=22, bias=True)
    (25): PReLU(num_parameters=1)
    (26): Linear(in_features=22, out_features=13, bias=True)
    (27): PReLU(num_parameters=1)
    (28): Linear(in_features=13, out_features=1, bias=True)
    (29): GELU(approximate='none')
  )
)
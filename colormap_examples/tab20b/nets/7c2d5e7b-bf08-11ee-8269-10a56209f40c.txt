RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=23, bias=True)
    (1): CELU(alpha=1.0)
    (2): Linear(in_features=23, out_features=84, bias=True)
    (3): Sigmoid()
    (4): Linear(in_features=84, out_features=177, bias=True)
    (5): Hardshrink(0.5)
    (6): Linear(in_features=177, out_features=121, bias=True)
    (7): LeakyReLU(negative_slope=0.01)
    (8): Linear(in_features=121, out_features=52, bias=True)
    (9): GELU(approximate='none')
    (10): Linear(in_features=52, out_features=84, bias=True)
    (11): ReLU6()
    (12): Linear(in_features=84, out_features=100, bias=True)
    (13): LogSigmoid()
    (14): Linear(in_features=100, out_features=111, bias=True)
    (15): Hardshrink(0.5)
    (16): Linear(in_features=111, out_features=185, bias=True)
    (17): RReLU(lower=0.125, upper=0.3333333333333333)
    (18): Linear(in_features=185, out_features=87, bias=True)
    (19): LogSigmoid()
    (20): Linear(in_features=87, out_features=198, bias=True)
    (21): SiLU()
    (22): Linear(in_features=198, out_features=8, bias=True)
    (23): Hardtanh(min_val=-1.0, max_val=1.0)
    (24): Linear(in_features=8, out_features=119, bias=True)
    (25): Tanhshrink()
    (26): Linear(in_features=119, out_features=62, bias=True)
    (27): Hardtanh(min_val=-1.0, max_val=1.0)
    (28): Linear(in_features=62, out_features=22, bias=True)
    (29): Tanh()
    (30): Linear(in_features=22, out_features=79, bias=True)
    (31): Mish()
    (32): Linear(in_features=79, out_features=23, bias=True)
    (33): ReLU()
    (34): Linear(in_features=23, out_features=1, bias=True)
    (35): LogSigmoid()
  )
)
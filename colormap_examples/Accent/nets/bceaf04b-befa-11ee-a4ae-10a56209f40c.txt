RandomNetwork(
  (layers): Sequential(
    (0): Linear(in_features=3, out_features=33, bias=True)
    (1): CELU(alpha=1.0)
    (2): Linear(in_features=33, out_features=143, bias=True)
    (3): Mish()
    (4): Linear(in_features=143, out_features=123, bias=True)
    (5): Tanh()
    (6): Linear(in_features=123, out_features=89, bias=True)
    (7): LeakyReLU(negative_slope=0.01)
    (8): Linear(in_features=89, out_features=80, bias=True)
    (9): GELU(approximate='none')
    (10): Linear(in_features=80, out_features=117, bias=True)
    (11): Tanh()
    (12): Linear(in_features=117, out_features=14, bias=True)
    (13): Sigmoid()
    (14): Linear(in_features=14, out_features=43, bias=True)
    (15): Tanhshrink()
    (16): Linear(in_features=43, out_features=103, bias=True)
    (17): ReLU6()
    (18): Linear(in_features=103, out_features=134, bias=True)
    (19): ReLU6()
    (20): Linear(in_features=134, out_features=105, bias=True)
    (21): Hardtanh(min_val=-1.0, max_val=1.0)
    (22): Linear(in_features=105, out_features=78, bias=True)
    (23): Hardtanh(min_val=-1.0, max_val=1.0)
    (24): Linear(in_features=78, out_features=6, bias=True)
    (25): PReLU(num_parameters=1)
    (26): Linear(in_features=6, out_features=1, bias=True)
    (27): RReLU(lower=0.125, upper=0.3333333333333333)
  )
)